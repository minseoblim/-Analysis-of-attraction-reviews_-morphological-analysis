{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python --version\n",
    "!pip install tqdm\n",
    "!pip install JPype1-0.7.1-cp37-cp37m-win_amd64.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 네이버에서 검색어 입력받아 검색 한 후 블로그 메뉴를 선택하고\n",
    "# 오른쪽에 있는 검색옵션 버튼을 눌러서\n",
    "# 정렬 방식과 기간을 입력하기\n",
    "\n",
    "#Step 0. 필요한 모듈과 라이브러리를 로딩하고 검색어를 입력 받습니다.\n",
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "import time\n",
    "import tqdm\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 분석 키워드 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "List = [#'키워드 설정']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for query_txt in List:\n",
    "# query_txt2 = input('2.제외할 첫번째 키워드를 입력하세요: ')\n",
    "# query_txt3 = input('3.제외할 두번째 키워드를 입력하세요: '|)\n",
    "    print (query_txt)\n",
    "start_date = 20190101 #input('4.조회를 시작할 날짜를 입력하세요(예:2017-01-01) :')\n",
    "end_date = 20200801 #input('5.조회를 종료할 날짜를 입력하세요(예:2017-12-31): ')\n",
    "print(start_date)\n",
    "print(end_date)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 키워드 제목, URL 크롤링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for query_txt in List:\n",
    "# query_txt2 = input('2.제외할 첫번째 키워드를 입력하세요: ')\n",
    "# query_txt3 = input('3.제외할 두번째 키워드를 입력하세요: '|)\n",
    "    print (query_txt)\n",
    "    start_date = \"20190101\" #input('4.조회를 시작할 날짜를 입력하세요(예:2017-01-01) :')\n",
    "    end_date = \"20200801\" #input('5.조회를 종료할 날짜를 입력하세요(예:2017-12-31): ')\n",
    "#     print(start_date)\n",
    "#     print(end_date)\n",
    "#Step 1. 크롬 웹브라우저 실행\n",
    "    path = \"chromedriver.exe\"\n",
    "\n",
    "    driver = webdriver.Chrome(executable_path='(driver) chromedriver.exe')\n",
    "# 사이트 주소\n",
    "    driver.get('http://www.naver.com')\n",
    "    time.sleep(2)\n",
    "\n",
    "#Step 2. 네이버 검색창에 \"검색어\" 검색\n",
    "    element = driver.find_element_by_id(\"query\")\n",
    "    element.send_keys(query_txt) \n",
    "    element.submit()\n",
    "\n",
    "#Step 3. \"블로그\" 카테고리 선택\n",
    "    driver.find_element_by_link_text(\"블로그\").click( )    # .click() 괄호 안을 눌러라는 뜻\n",
    "\n",
    "#Step 4. 오른쪽의 검색 옵션 버튼 클릭\n",
    "    driver.find_element_by_id(\"_search_option_btn\").click( )\n",
    "\n",
    "#Step 5. 정렬 : \"관련도순\" \n",
    "    driver.find_element_by_xpath(\"\"\"//*[@id=\"snb\"]/div/ul/li[1]/a\"\"\").click( )  # 정렬 버튼의 xpath 클릭\n",
    "    driver.find_element_by_xpath(\"\"\"//*[@id=\"snb\"]/div/ul/li[1]/div/ul/li[1]/a\"\"\").click( ) # 관련도순 xpath\n",
    "\n",
    "#Step 6. 날짜 입력\n",
    "    driver.find_element_by_xpath(\"\"\"//*[@id=\"snb\"]/div/ul/li[2]/a\"\"\").click()\n",
    "    time.sleep(2)\n",
    "\n",
    "# 시작 날짜 입력하기\n",
    "    s_date = driver.find_element_by_xpath(\"\"\"//*[@id=\"blog_input_period_begin\"]\"\"\")     \n",
    "    driver.find_element_by_xpath(\"\"\"//*[@id=\"blog_input_period_begin\"]\"\"\").click()\n",
    "    s_date.clear( )  # 날짜 입력 부분에 기존에 입력되어 있던 날짜를 제거합니다. \n",
    "    time.sleep(1)\n",
    "# 아래 코드가 날짜를 for 반복문으로 1 글자씩 입력하는 부분입니다.\n",
    "    for c in start_date:\n",
    "        s_date.send_keys(c)\n",
    "        time.sleep(0.1)\n",
    "    \n",
    "# 종료 날짜 입력하기\n",
    "    e_date = driver.find_element_by_xpath(\"\"\"//*[@id=\"blog_input_period_end\"]\"\"\")\n",
    "    driver.find_element_by_xpath(\"\"\"//*[@id=\"blog_input_period_end\"]\"\"\").click()\n",
    "    e_date.clear()\n",
    "    time.sleep(1)\n",
    "\n",
    "    for c in end_date:\n",
    "        e_date.send_keys(c)\n",
    "        time.sleep(0.1)\n",
    "    \n",
    "# Step7. 날짜 입력 \"적용하기\" 버튼을 클릭 합니다.  \n",
    "    driver.find_element_by_class_name(\"tx\").click()\n",
    "    time.sleep(1)\n",
    "\n",
    "# # Step 8. 상세 검색버튼을 클릭 후 제외할 단어들을 설정합니다.\n",
    "# ele2 = driver.find_element_by_id(\"inpop3\")\n",
    "# ele2.send_keys(query_txt2)\n",
    "# ele2.send_keys(',')\n",
    "# ele2.send_keys(query_txt3)\n",
    "# driver.find_element_by_css_selector(\".btn_ft.ty_green._search\").click( )\n",
    "    url_list = []\n",
    "    title_list = []\n",
    "\n",
    "# 몇개의 페이지를 크롤링할지 선택\n",
    "    total_page = 10\n",
    "    for i in tqdm(range(0, total_page)):  # 페이지 번호\n",
    "        i = i*10 + 1\n",
    "        url = \"https://search.naver.com/search.naver\\\n",
    "?date_from={0}&date_option=8&date_to={1}\\\n",
    "&dup_remove=1&nso=p%3Afrom{2}to{3}post_blogurl=\\\n",
    "&post_blogurl_without=&query={4}&sm=tab_pge&srchby=all&st=sim&where=post&start={5}\".format(start_date,end_date,start_date,end_date,query_txt, i)\n",
    "        driver.get(url)\n",
    "        time.sleep(0.5)\n",
    "    \n",
    "    # URL 크롤링 시작\n",
    "        titles = \"a.sh_blog_title._sp_each_url._sp_each_title\"\n",
    "        article_raw = driver.find_elements_by_css_selector(titles)\n",
    "#     article_raw\n",
    "\n",
    "    # url 크롤링 시작    \n",
    "        for article in article_raw:\n",
    "            url = article.get_attribute('href')   \n",
    "            url_list.append(url)\n",
    "    \n",
    "    # 제목 크롤링 시작    \n",
    "        for article in article_raw:\n",
    "            title = article.get_attribute('title')   \n",
    "            title_list.append(title)\n",
    "    \n",
    "            print(title)\n",
    "    \n",
    "    print('url갯수: ', len(url_list))\n",
    "    print('url갯수: ', len(title_list))\n",
    "\n",
    "    df = pd.DataFrame({'url':url_list, 'title':title_list})\n",
    "\n",
    "# 저장하기\n",
    "    df.to_excel(\"blog_url_{0}.xlsx\".format(query_txt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 블로그 본문 크롤링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"url_list.csv\" 불러오기\n",
    "for query_txt in List:\n",
    "    url_load = pd.read_excel(\"blog_url_{0}.xlsx\".format(query_txt))        # 기본 모델\n",
    "\n",
    "#     num_list = len(url_load)\n",
    "\n",
    "#     print(num_list)\n",
    "#     url_load.head()\n",
    "    dict = {}  # 전체 크롤링 데이터를 담을 그릇\n",
    "\n",
    "# ★수집할 글 갯수\n",
    "    number = 100\n",
    "    for i in tqdm(range(0, number)): \n",
    "    # 글 띄우기\n",
    "        url = url_load['url'][i]\n",
    "        driver = webdriver.Chrome(executable_path='(driver) chromedriver.exe')\n",
    "        driver.get(url)   # 글 띄우기\n",
    "    \n",
    "    # 크롤링\n",
    "    \n",
    "        try : \n",
    "            # iframe 접근\n",
    "            driver.switch_to_frame('mainFrame')\n",
    "\n",
    "            target_info = {}\n",
    "\n",
    "            # 제목 크롤링 시작\n",
    "            overlays = \".se-fs-.se-ff-\"                                 \n",
    "            tit = driver.find_element_by_css_selector(overlays)         # title\n",
    "            title = tit.text\n",
    "            title\n",
    "\n",
    "#         # 글쓴이 크롤링 시작\n",
    "#         overlays = \".nick\"                                 \n",
    "#         nick = driver.find_element_by_css_selector(overlays)         # nick\n",
    "#         nickname = nick.text\n",
    "\n",
    "            # 날짜 크롤링\n",
    "            overlays = \".se_publishDate.pcol2\"                                 \n",
    "            date = driver.find_element_by_css_selector(overlays)         # date\n",
    "            datetime = date.text\n",
    "\n",
    "            # 내용 크롤링\n",
    "            overlays = \".se-component.se-text.se-l-default\"                                 \n",
    "            contents = driver.find_elements_by_css_selector(overlays)         # date\n",
    "\n",
    "            content_list = []\n",
    "            for content in contents:\n",
    "                content_list.append(content.text)\n",
    "\n",
    "            content_str = ' '.join(content_list)\n",
    "\n",
    "            # 글 하나는 target_info라는 딕셔너리에 담기게 되고,\n",
    "            target_info['title'] = title\n",
    "    #         target_info['nickname'] = nickname\n",
    "            target_info['datetime'] = datetime\n",
    "            target_info['content'] = content_str\n",
    "\n",
    "            # 각각의 글은 dict라는 딕셔너리에 담기게 됩니다.\n",
    "            dict[i] = target_info\n",
    "            time.sleep(1)\n",
    "        \n",
    "            print(i, title)\n",
    "\n",
    "            # 글 하나 크롤링 후 크롬 창 닫기\n",
    "            driver.close()       \n",
    "    \n",
    "        # 에러나면 현재 크롬창 닫고 다음 글(i+1)로 이동\n",
    "        except:\n",
    "            driver.close()\n",
    "            time.sleep(1)\n",
    "            continue\n",
    "    \n",
    "        # 중간,중간에 파일로 저장하기\n",
    "        if i % 20 == 0 :\n",
    "            # 판다스로 만들기\n",
    "            import pandas as pd\n",
    "            result_df = pd.DataFrame.from_dict(dict, 'index')\n",
    "\n",
    "            # 저장하기\n",
    "            result_df.to_excel(\"blog_content_{0}.xlsx\".format(query_txt))\n",
    "            time.sleep(3)\n",
    "    \n",
    "    print('수집한 글 갯수: ', len(dict))\n",
    "    # print(dict)    \n",
    "    \n",
    "    # 판다스로 만들기\n",
    "    result_df = pd.DataFrame.from_dict(dict, 'index')\n",
    "\n",
    "    # 저장하기\n",
    "    result_df.to_excel(\"blog_content_{0}.xlsx\".format(query_txt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 판다스 통합"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_data1 = pd.read_excel(\"blog_content_동두천자연휴양림.xlsx\")\n",
    "temp_data2 = pd.read_excel(\"blog_content_한탄강주상절리길.xlsx\")\n",
    "temp_data3 = pd.read_excel(\"blog_content_여강길.xlsx\")\n",
    "temp_data4 = pd.read_excel(\"blog_content_잣향기푸른숲.xlsx\")\n",
    "temp_data5 = pd.read_excel(\"blog_content_바람새마을 소풍정원.xlsx\")\n",
    "temp_data6 = pd.read_excel(\"blog_content_평화누리공원.xlsx\")\n",
    "temp_data7 = pd.read_excel(\"blog_content_행주산성역사공원.xlsx\")\n",
    "temp_data8 = pd.read_excel(\"blog_content_갯골생태공원.xlsx\")\n",
    "temp_data9 = pd.read_excel(\"blog_content_김포 평화누리길 1코스.xlsx\")\n",
    "temp_data10 = pd.read_excel(\"blog_content_곤지암.xlsx\")\n",
    "\n",
    "temp_data1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 내용 텍스트 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install graphviz\n",
    "!pip install konlpy\n",
    "\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 그래프\n",
    "%matplotlib inline\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import graphviz             \n",
    "from sklearn.tree import export_graphviz\n",
    "\n",
    "# 그래프에서 한글 폰트 깨지는 문제에 대한 대처(전역 글꼴 설정)\n",
    "from matplotlib import font_manager, rc\n",
    "font_name = font_manager.FontProperties(fname='c:/Windows/Fonts/malgun.ttf').get_name()\n",
    "rc('font', family=font_name)\n",
    "\n",
    "# 워닝 없애주는 것\n",
    "import warnings  \n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 형태소 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#형태소 분석\n",
    "from konlpy.tag import Kkma       ; kkma = Kkma()\n",
    "from konlpy.tag import Hannanum   ; hannanum = Hannanum()\n",
    "from konlpy.tag import Okt        ; t = Okt()\n",
    "from konlpy.tag import *\n",
    "from konlpy.tag import Twitter\n",
    "import pickle\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for query_txt in List:\n",
    "    raw = pd.read_excel(\"blog_content_{0}.xlsx\".format(query_txt)) \n",
    "    content_list = raw.content.values.tolist()\n",
    "    content_list\n",
    "    print(len(content_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for query_txt in List:\n",
    "    raw = pd.read_excel(\"blog_content_{0}.xlsx\".format(query_txt)) \n",
    "    content_list = raw.content.values.tolist()\n",
    "\n",
    "    print(len(content_list))\n",
    "    content_list\n",
    "    # 리스트 중 str 타입이 아닌 요소들이 존재함을 확인\n",
    "    for i in content_list:\n",
    "        if type(i) == float:\n",
    "            i = str(i)\n",
    "#             print(i)\n",
    "    # str 타입이 아닌 요소들이 있으면 전부 str 타입으로 바꿔라!\n",
    "    for i in range(len(content_list)):\n",
    "        if type(content_list[i]) != str:\n",
    "            content_list[i] = str(content_list[i])\n",
    "            \n",
    "    content_text = ''\n",
    "\n",
    "    for each_line in content_list[:2000]:\n",
    "        content_text = content_text + each_line + '\\n'\n",
    "    print(content_text)\n",
    "    \n",
    "    tokens_ko = t.morphs(content_text)\n",
    "    tokens_ko"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for query_txt in List:\n",
    "    raw = pd.read_excel(\"blog_content_{0}.xlsx\".format(query_txt)) \n",
    "    content_list = raw.content.values.tolist()\n",
    "\n",
    "    print(len(content_list))\n",
    "    content_list\n",
    "    # 리스트 중 str 타입이 아닌 요소들이 존재함을 확인\n",
    "    for i in content_list:\n",
    "        if type(i) == float:\n",
    "            i = str(i)\n",
    "            print(i)\n",
    "    # str 타입이 아닌 요소들이 있으면 전부 str 타입으로 바꿔라!\n",
    "    for i in range(len(content_list)):\n",
    "        if type(content_list[i]) != str:\n",
    "            content_list[i] = str(content_list[i])\n",
    "            \n",
    "    content_text = ''\n",
    "\n",
    "    for each_line in content_list[:2000]:\n",
    "        content_text = content_text + each_line + '\\n'\n",
    "    \n",
    "\n",
    "\n",
    "#     twitter = Twitter()\n",
    "#     raw_pos_tagged = twitter.pos(content_text, norm=True, stem=True) # POS Tagging\n",
    "#     raw_pos_tagged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_ko = t.morphs(content_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전체 갯수 확인\n",
    "ko = nltk.Text(tokens_ko)   \n",
    "print(len(ko.tokens))          # 토큰 전체 갯수\n",
    "print(len(set(ko.tokens)))     # 토큰 unique 갯수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ko = nltk.Text(tokens_ko)\n",
    "ko.vocab().most_common(100)    # 가장 많이 나온 단어 100개"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#불용어 제거\n",
    "# 불용어 : 인터넷 검색 시 검색 용어로 사용하지 않는 단어. 관사, 전치사, 조사, 접속사 등 검색 색인 단어로 의미가 없는 단어\n",
    "stop_words = [')','?','1','\"(', '_', ')/','\\n','.',',', '<','!','(','(', '??','..', '4', '|', '>', '?(', '\"…', '#', '&', '・', \"']\",'.',' ','/',\"'\",'’','”','“','·', '[','!','\\n','·','‘','\"','\\n ',']',':','…',')','(','-', 'nan','가','요','답변','...','을','수','에','질문','제','를','이','도',\n",
    "                      '좋','1','는','로','으로','2','것','은','다',',','니다','대','들',\n",
    "              '이다','하고','입니다','대한','에서','수사','심의',\n",
    "                      '2017','들','데','..','의','때','겠','고','게','네요','한','일','할',\n",
    "                      '10','?','하는','06','주','려고','인데','거','좀','는데','~','ㅎㅎ',\n",
    "                      '하나','이상','20','뭐','까','있는','잘','습니다','다면','했','주려',\n",
    "                      '지','있','못','후','중','줄','6','과','어떤','기본','!!',\n",
    "                      '단어','라고','중요한','합','가요','....','보이','네','무지',\n",
    "             '적', '성', '삼', '등', '전', '인', '그', '했다', '와', '위', '해', '권', '된', '서', '말', '분']\n",
    "\n",
    "tokens_ko = [each_word for each_word in tokens_ko\n",
    "             if each_word not in stop_words]\n",
    "\n",
    "ko = nltk.Text(tokens_ko)\n",
    "ko.vocab().most_common(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import graphviz\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "from PIL import Image\n",
    "from sklearn.tree import export_graphviz\n",
    "plt.figure(figsize=(15,6))\n",
    "ko.plot(50) \n",
    "plt.show()\n",
    "\n",
    "# 그래프에서 한글 폰트 깨지는 문제에 대한 대처(전역 글꼴 설정)\n",
    "from matplotlib import font_manager, rc\n",
    "font_name = font_manager.FontProperties(fname='c:/Windows/Fonts/malgun.ttf').get_name()\n",
    "rc('font', family=font_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = ko.vocab().most_common(300)\n",
    "\n",
    "print(len(data))\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list tuple을 딕셔너리로 만들어주는 함수\n",
    "def todict(list_tuple):    \n",
    "    todict = {}\n",
    "    for i in range(0,len(list_tuple)):\n",
    "        todict[data[i][0]] = data[i][1]\n",
    "    return todict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 워드클라우드를 그려보자\n",
    "wordcloud = WordCloud(font_path='c:/Windows/Fonts/malgun.ttf',\n",
    "                      relative_scaling = 0.2,\n",
    "                      #stopwords=STOPWORDS,\n",
    "                      background_color='white',\n",
    "                      ).generate_from_frequencies(todict(data))\n",
    "\n",
    "plt.figure(figsize=(16,8))\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
